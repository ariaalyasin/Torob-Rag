{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9172fbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from IPython.display import display\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "566ef87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Root: /home/roshan/Aria/mlops\n",
      "ğŸ“ Raw data: /home/roshan/Aria/mlops/Data\n",
      "ğŸ“ Processed data: /home/roshan/Aria/mlops/Data/processed\n",
      "ğŸ“ Outputs: /home/roshan/Aria/mlops/outputs\n"
     ]
    }
   ],
   "source": [
    "# --- Paths & config ---\n",
    "ROOT = Path(\"..\").resolve()\n",
    "CONFIG_PATH = ROOT / \"configs\" / \"config.yaml\"\n",
    "\n",
    "if CONFIG_PATH.exists():\n",
    "    with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "else:\n",
    "    config = {}\n",
    "\n",
    "DATA_RAW = ROOT / config.get(\"data\", {}).get(\"raw_path\", \"Data/\")\n",
    "DATA_PROCESSED = ROOT / config.get(\"data\", {}).get(\"processed_path\", \"Data/processed/\")\n",
    "\n",
    "OUTPUTS_DIR = ROOT / \"outputs\"\n",
    "EMB_DIR = OUTPUTS_DIR / \"embeddings\"\n",
    "VDB_DIR = OUTPUTS_DIR / \"vector_db\"\n",
    "IMG_CACHE_DIR = OUTPUTS_DIR / \"image_cache\"\n",
    "\n",
    "for p in [OUTPUTS_DIR, EMB_DIR, VDB_DIR, IMG_CACHE_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ Root:\", ROOT)\n",
    "print(\"ğŸ“ Raw data:\", DATA_RAW)\n",
    "print(\"ğŸ“ Processed data:\", DATA_PROCESSED)\n",
    "print(\"ğŸ“ Outputs:\", OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0804cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text processing utilities ready\n"
     ]
    }
   ],
   "source": [
    "# ===== Text Processing Utilities =====\n",
    "\n",
    "def ensure_string(x: Any) -> str:\n",
    "    \"\"\"Convert any value to string, handling None and NaN.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "\n",
    "def normalize_persian_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Persian/Arabic text for consistent processing.\n",
    "    - Converts Arabic variants (ÙŠ, Ùƒ) to Persian (ÛŒ, Ú©)\n",
    "    - Removes extra whitespace\n",
    "    - Ensures deterministic output\n",
    "    \"\"\"\n",
    "    text = ensure_string(text)\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize Arabic to Persian characters\n",
    "    text = text.replace(\"ÙŠ\", \"ÛŒ\").replace(\"Ùƒ\", \"Ú©\")\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def build_product_text(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Build comprehensive product text for embedding.\n",
    "    Combines name, brand, and category into single text.\n",
    "    Skips unidentified/noise categories and brands (empty or \"Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯Ù‡\").\n",
    "    Note: extra_features are NOT included as they don't provide useful semantic information.\n",
    "    \"\"\"\n",
    "    name = normalize_persian_text(row.get(\"persian_name\", \"\"))\n",
    "    brand = normalize_persian_text(row.get(\"brand_title\", \"\"))\n",
    "    category = normalize_persian_text(row.get(\"category_title\", \"\"))\n",
    "    \n",
    "    # Filter out noise/unidentified labels\n",
    "    noise_keywords = [\"Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯Ù‡\", \"not detected\", \"unknown\", \"n/a\"]\n",
    "    \n",
    "    if any(keyword in brand.lower() for keyword in noise_keywords):\n",
    "        brand = \"\"\n",
    "    if any(keyword in category.lower() for keyword in noise_keywords):\n",
    "        category = \"\"\n",
    "    \n",
    "    # Build parts list with only meaningful values\n",
    "    parts = []\n",
    "    if name:\n",
    "        parts.append(name)\n",
    "    if brand:\n",
    "        parts.append(f\"Ø¨Ø±Ù†Ø¯: {brand}\")\n",
    "    if category:\n",
    "        parts.append(f\"Ø¯Ø³ØªÙ‡: {category}\")\n",
    "    \n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "print(\"âœ… Text processing utilities ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70b0aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading cleaned datasets...\n",
      "âœ… Loaded base_products: (1022296, 8)\n",
      "âœ… Loaded brands: (2025, 2)\n",
      "âœ… Loaded categories: (746, 3)\n",
      "âœ… Enriched with brand titles\n",
      "âœ… Enriched with category titles\n",
      "\n",
      "ğŸ§¹ Filtering unidentified categories and brands...\n",
      "   âœ“ Filtered 844,870 products with unidentified brand (id=-1)\n",
      "\n",
      "ğŸ“‹ Sample enriched products:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_key</th>\n",
       "      <th>persian_name</th>\n",
       "      <th>brand_title</th>\n",
       "      <th>category_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chjknu</td>\n",
       "      <td>ÙØ±Ø´ 700Ø´Ø§Ù†Ù‡ Ø§Ù¾Ø§Ù„ ÙÛŒØ±ÙˆØ²Ù‡ Ø§ÛŒ</td>\n",
       "      <td></td>\n",
       "      <td>ÙØ±Ø´ Ù…Ø§Ø´ÛŒÙ†ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppndkv</td>\n",
       "      <td>ØµÙ†Ø¯Ù„ÛŒ ØªØ§Ø¨ Ù…Ø¯Ù„ chioco</td>\n",
       "      <td></td>\n",
       "      <td>Ø³Ø§ÛŒØ± ØµÙ†Ø¯Ù„ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lwtjlj</td>\n",
       "      <td>Ø¨Ø´Ù‚Ø§Ø¨ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©ÛŒÚ© ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡Ø³Ø§Ø²ÛŒ Ø¨Ø§ Ø³ÛŒØ³ØªÙ…Ø¹Ø§Ù…Ù„ 13 Ù‚...</td>\n",
       "      <td>Ø§Ù†Ú¯Ù„ÛŒØ´ Ù‡ÙˆÙ… / English Home</td>\n",
       "      <td>Ø¯ÛŒØ³ Ùˆ Ø¨Ø´Ù‚Ø§Ø¨</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  random_key                                       persian_name  \\\n",
       "0     chjknu                         ÙØ±Ø´ 700Ø´Ø§Ù†Ù‡ Ø§Ù¾Ø§Ù„ ÙÛŒØ±ÙˆØ²Ù‡ Ø§ÛŒ   \n",
       "1     ppndkv                               ØµÙ†Ø¯Ù„ÛŒ ØªØ§Ø¨ Ù…Ø¯Ù„ chioco   \n",
       "2     lwtjlj  Ø¨Ø´Ù‚Ø§Ø¨ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©ÛŒÚ© ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡Ø³Ø§Ø²ÛŒ Ø¨Ø§ Ø³ÛŒØ³ØªÙ…Ø¹Ø§Ù…Ù„ 13 Ù‚...   \n",
       "\n",
       "                 brand_title category_title  \n",
       "0                                ÙØ±Ø´ Ù…Ø§Ø´ÛŒÙ†ÛŒ  \n",
       "1                                Ø³Ø§ÛŒØ± ØµÙ†Ø¯Ù„ÛŒ  \n",
       "2  Ø§Ù†Ú¯Ù„ÛŒØ´ Ù‡ÙˆÙ… / English Home    Ø¯ÛŒØ³ Ùˆ Ø¨Ø´Ù‚Ø§Ø¨  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load cleaned base products\n",
    "BASE_PRODUCTS_PATH = DATA_PROCESSED / \"base_products_clean.parquet\"\n",
    "BRANDS_PATH = DATA_PROCESSED / \"brands_clean.parquet\"\n",
    "CATEGORIES_PATH = DATA_PROCESSED / \"categories_clean.parquet\"\n",
    "\n",
    "if not BASE_PRODUCTS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"âŒ Cleaned data not found: {BASE_PRODUCTS_PATH}\\n\"\n",
    "        \"Please run notebook 02 (Data Cleaning) first to generate processed files.\"\n",
    "    )\n",
    "\n",
    "print(\"ğŸ“‚ Loading cleaned datasets...\")\n",
    "base_products = pd.read_parquet(BASE_PRODUCTS_PATH)\n",
    "print(f\"âœ… Loaded base_products: {base_products.shape}\")\n",
    "\n",
    "# Load reference tables\n",
    "brands = pd.read_parquet(BRANDS_PATH) if BRANDS_PATH.exists() else pd.read_parquet(DATA_RAW / \"brands.parquet\")\n",
    "categories = pd.read_parquet(CATEGORIES_PATH) if CATEGORIES_PATH.exists() else pd.read_parquet(DATA_RAW / \"categories.parquet\")\n",
    "\n",
    "print(f\"âœ… Loaded brands: {brands.shape}\")\n",
    "print(f\"âœ… Loaded categories: {categories.shape}\")\n",
    "\n",
    "# Create mapping dictionaries\n",
    "brands_map = brands[[\"id\", \"title\"]].drop_duplicates(subset=[\"id\"]).rename(\n",
    "    columns={\"id\": \"brand_id\", \"title\": \"brand_title\"}\n",
    ")\n",
    "categories_map = categories[[\"id\", \"title\"]].drop_duplicates(subset=[\"id\"]).rename(\n",
    "    columns={\"id\": \"category_id\", \"title\": \"category_title\"}\n",
    ")\n",
    "\n",
    "# Enrich products with readable titles\n",
    "if \"brand_id\" in base_products.columns:\n",
    "    base_products = base_products.merge(brands_map, on=\"brand_id\", how=\"left\")\n",
    "    print(\"âœ… Enriched with brand titles\")\n",
    "\n",
    "if \"category_id\" in base_products.columns:\n",
    "    base_products = base_products.merge(categories_map, on=\"category_id\", how=\"left\")\n",
    "    print(\"âœ… Enriched with category titles\")\n",
    "\n",
    "# Filter out unidentified/noise categories and brands\n",
    "print(\"\\nğŸ§¹ Filtering unidentified categories and brands...\")\n",
    "\n",
    "# Replace unidentified categories (id=-1) with empty string\n",
    "if \"category_id\" in base_products.columns:\n",
    "    unidentified_cat_mask = base_products[\"category_id\"] == -1\n",
    "    if unidentified_cat_mask.any():\n",
    "        base_products.loc[unidentified_cat_mask, \"category_title\"] = \"\"\n",
    "        print(f\"   âœ“ Filtered {unidentified_cat_mask.sum():,} products with unidentified category (id=-1)\")\n",
    "\n",
    "# Replace unidentified brands (id=-1) with empty string  \n",
    "if \"brand_id\" in base_products.columns:\n",
    "    unidentified_brand_mask = base_products[\"brand_id\"] == -1\n",
    "    if unidentified_brand_mask.any():\n",
    "        base_products.loc[unidentified_brand_mask, \"brand_title\"] = \"\"\n",
    "        print(f\"   âœ“ Filtered {unidentified_brand_mask.sum():,} products with unidentified brand (id=-1)\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nğŸ“‹ Sample enriched products:\")\n",
    "display(base_products[[\"random_key\", \"persian_name\", \"brand_title\", \"category_title\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5832ad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "ğŸ¤– Text Embedding Configuration:\n",
      "   Model: sentence-transformers/all-MiniLM-L12-v2\n",
      "   Device: cuda\n",
      "   Batch size: 64\n",
      "\n",
      "â³ Loading text model...\n",
      "âœ… Text model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import text embedding dependencies\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "\n",
    "# Model configuration\n",
    "TEXT_MODEL_NAME = config.get(\"feature_engineering\", {}).get(\"text\", {}).get(\n",
    "    \"model_name\", \"abbas\"\n",
    ")\n",
    "TEXT_BATCH_SIZE = config.get(\"feature_engineering\", {}).get(\"text\", {}).get(\n",
    "    \"batch_size\", 64\n",
    ")\n",
    "\n",
    "# Device selection\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "print(f\"\\nğŸ¤– Text Embedding Configuration:\")\n",
    "print(f\"   Model: {TEXT_MODEL_NAME}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Batch size: {TEXT_BATCH_SIZE}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nâ³ Loading text model...\")\n",
    "text_model = SentenceTransformer(TEXT_MODEL_NAME, device=DEVICE)\n",
    "print(\"âœ… Text model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "338929ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ Preparing working dataset (sample=None)...\n",
      "âœ… Using all 1,022,296 products\n",
      "â³ Building product texts...\n",
      "âœ… Product texts created: 1,022,296\n",
      "\n",
      "ğŸ“ Example product text:\n",
      "ØµÙ†Ø¯Ù„ÛŒ ØªØ§Ø¨ Ù…Ø¯Ù„ chioco | Ø¯Ø³ØªÙ‡: Ø³Ø§ÛŒØ± ØµÙ†Ø¯Ù„ÛŒ...\n"
     ]
    }
   ],
   "source": [
    "# Prepare working dataset\n",
    "# For demonstration, use a sample. For production, process full dataset in batches.\n",
    "SAMPLE_SIZE = None  # Set to None for full dataset\n",
    "\n",
    "print(f\"\\nğŸ“‹ Preparing working dataset (sample={SAMPLE_SIZE})...\")\n",
    "\n",
    "# Select relevant columns (excluding extra_features - not useful for embeddings)\n",
    "cols = [\"random_key\", \"persian_name\"]\n",
    "for optional_col in [\"brand_title\", \"category_title\", \"image_url\"]:\n",
    "    if optional_col in base_products.columns:\n",
    "        cols.append(optional_col)\n",
    "\n",
    "work_df = base_products[cols].copy()\n",
    "\n",
    "# Sample if needed\n",
    "if SAMPLE_SIZE is not None and len(work_df) > SAMPLE_SIZE:\n",
    "    work_df = work_df.sample(n=SAMPLE_SIZE, random_state=RANDOM_SEED)\n",
    "    print(f\"âœ… Sampled {SAMPLE_SIZE} products from {len(base_products):,}\")\n",
    "else:\n",
    "    print(f\"âœ… Using all {len(work_df):,} products\")\n",
    "\n",
    "work_df = work_df.reset_index(drop=True)\n",
    "\n",
    "# Build product text for each row\n",
    "print(\"â³ Building product texts...\")\n",
    "work_df[\"product_text\"] = work_df.apply(build_product_text, axis=1)\n",
    "\n",
    "print(f\"âœ… Product texts created: {len(work_df):,}\")\n",
    "print(f\"\\nğŸ“ Example product text:\")\n",
    "print(work_df[\"product_text\"].iloc[1][:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57ff1af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â³ Generating text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15974/15974 [05:03<00:00, 52.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text embeddings generated:\n",
      "   Shape: (1022296, 384)\n",
      "   Dtype: float32\n",
      "   Sample norm: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Generate text embeddings\n",
    "print(\"\\nâ³ Generating text embeddings...\")\n",
    "\n",
    "# MiniLM doesn't use prefixes - use text directly\n",
    "texts = work_df[\"product_text\"].tolist()\n",
    "\n",
    "# Encode with normalization\n",
    "text_embeddings = text_model.encode(\n",
    "    texts,\n",
    "    batch_size=TEXT_BATCH_SIZE,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "text_embeddings = np.asarray(text_embeddings, dtype=np.float32)\n",
    "\n",
    "print(f\"âœ… Text embeddings generated:\")\n",
    "print(f\"   Shape: {text_embeddings.shape}\")\n",
    "print(f\"   Dtype: {text_embeddings.dtype}\")\n",
    "print(f\"   Sample norm: {float(np.linalg.norm(text_embeddings[0])):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "532f9d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Stratified Image Sampling (preserving category distribution)...\n",
      "   Target: 10,000 images\n",
      "   Products with images: 1,018,955\n",
      "   Categories with â‰¥2 samples: 1,018,947 products\n",
      "   Categories with 1 sample: 8 products\n",
      "   âœ… Sampled 10,000 images with stratification\n",
      "\n",
      "   ğŸ“ˆ Category Distribution (Top 10):\n",
      "      ØªØ§Ø¨Ù„Ùˆ Ùˆ Ù…Ø¬Ø³Ù…Ù‡: 378 (3.8%)\n",
      "      ÙØ±Ø´ Ù…Ø§Ø´ÛŒÙ†ÛŒ: 281 (2.8%)\n",
      "      Ø±ÙˆÙ…ÛŒØ²ÛŒ Ùˆ Ù¾Ø±Ø¯Ù‡: 204 (2.0%)\n",
      "      Ø§Ù†ÙˆØ§Ø¹ Ù¾Ø§Ø±Ú†Ù‡: 185 (1.8%)\n",
      "      Ø±ÙˆØªØ®ØªÛŒØŒ Ù„Ø­Ø§Ù Ùˆ Ø³Ø±ÙˆÛŒØ³ Ø®ÙˆØ§Ø¨: 179 (1.8%)\n",
      "      Ù…Ø§Ú¯: 178 (1.8%)\n",
      "      Ø§Ø´ÛŒØ§Ø¡ Ù‚Ø¯ÛŒÙ…ÛŒ Ùˆ Ú©Ù„Ú©Ø³ÛŒÙˆÙ†ÛŒ: 146 (1.5%)\n",
      "      Ø³Ø§Ø¹Øª Ø¯ÛŒÙˆØ§Ø±ÛŒÙ¬ Ø±ÙˆÙ…ÛŒØ²ÛŒ Ùˆ ØªØ²Ø¦ÛŒÙ†ÛŒ: 145 (1.5%)\n",
      "      Ù„ÙˆØ§Ø²Ù… ÛŒØ¯Ú©ÛŒ Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡: 140 (1.4%)\n",
      "      Ù¾Ø§Ø±Ú† Ùˆ Ù„ÛŒÙˆØ§Ù†: 137 (1.4%)\n"
     ]
    }
   ],
   "source": [
    "# Stratified sampling for images to preserve category distribution\n",
    "MAX_IMAGES = 10000  # Maximum number of images to download and embed\n",
    "\n",
    "if \"image_url\" in work_df.columns and \"category_title\" in work_df.columns:\n",
    "    print(f\"\\nğŸ“Š Stratified Image Sampling (preserving category distribution)...\")\n",
    "    print(f\"   Target: {MAX_IMAGES:,} images\")\n",
    "    \n",
    "    # Filter products that have valid image URLs\n",
    "    products_with_images = work_df[work_df[\"image_url\"].notna() & (work_df[\"image_url\"] != \"\")].copy()\n",
    "    \n",
    "    print(f\"   Products with images: {len(products_with_images):,}\")\n",
    "    \n",
    "    if len(products_with_images) > MAX_IMAGES:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # Use category_title for stratification (handle missing categories)\n",
    "        products_with_images[\"_strata\"] = products_with_images[\"category_title\"].fillna(\"_unknown\")\n",
    "        \n",
    "        # Count samples per category\n",
    "        category_counts = products_with_images[\"_strata\"].value_counts()\n",
    "        \n",
    "        # Separate categories with only 1 sample (can't stratify these)\n",
    "        single_sample_categories = category_counts[category_counts == 1].index.tolist()\n",
    "        multi_sample_mask = ~products_with_images[\"_strata\"].isin(single_sample_categories)\n",
    "        \n",
    "        products_multi = products_with_images[multi_sample_mask].copy()\n",
    "        products_single = products_with_images[~multi_sample_mask].copy()\n",
    "        \n",
    "        print(f\"   Categories with â‰¥2 samples: {len(products_multi):,} products\")\n",
    "        print(f\"   Categories with 1 sample: {len(products_single):,} products\")\n",
    "        \n",
    "        # Calculate how many to sample from multi-sample categories\n",
    "        n_from_multi = min(MAX_IMAGES, len(products_multi))\n",
    "        \n",
    "        if len(products_multi) > 0 and n_from_multi > 0:\n",
    "            # Stratified sample from multi-sample categories\n",
    "            sampled_multi, _ = train_test_split(\n",
    "                products_multi,\n",
    "                train_size=n_from_multi,\n",
    "                stratify=products_multi[\"_strata\"],\n",
    "                random_state=RANDOM_SEED\n",
    "            )\n",
    "        else:\n",
    "            sampled_multi = pd.DataFrame()\n",
    "        \n",
    "        # Add single-sample categories if we have room\n",
    "        remaining_slots = MAX_IMAGES - len(sampled_multi)\n",
    "        if remaining_slots > 0 and len(products_single) > 0:\n",
    "            n_single = min(remaining_slots, len(products_single))\n",
    "            sampled_single = products_single.sample(n=n_single, random_state=RANDOM_SEED)\n",
    "            sampled_images = pd.concat([sampled_multi, sampled_single], ignore_index=False)\n",
    "        else:\n",
    "            sampled_images = sampled_multi\n",
    "        \n",
    "        sampled_images = sampled_images.drop(columns=[\"_strata\"])\n",
    "        \n",
    "        print(f\"   âœ… Sampled {len(sampled_images):,} images with stratification\")\n",
    "        \n",
    "        # Show category distribution\n",
    "        print(f\"\\n   ğŸ“ˆ Category Distribution (Top 10):\")\n",
    "        category_dist = sampled_images[\"category_title\"].value_counts().head(10)\n",
    "        for cat, count in category_dist.items():\n",
    "            cat_display = cat if cat else \"(no category)\"\n",
    "            pct = (count / len(sampled_images)) * 100\n",
    "            print(f\"      {cat_display}: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        sampled_images = products_with_images\n",
    "        print(f\"   âœ… Using all {len(sampled_images):,} products with images (less than {MAX_IMAGES:,})\")\n",
    "    \n",
    "    # Store the indices of products selected for image embedding\n",
    "    image_sample_indices = sampled_images.index.tolist()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Skipping image sampling - no image_url or category_title column\")\n",
    "    image_sample_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08c1cfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ–¼ï¸  Image Embedding Configuration:\n",
      "   Model: openai/clip-vit-base-patch32\n",
      "   Device: cuda\n",
      "   Cache directory: /home/roshan/Aria/mlops/outputs/image_cache\n",
      "\n",
      "â³ Loading CLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CLIP model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import image embedding dependencies\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "IMAGE_MODEL_NAME = config.get(\"feature_engineering\", {}).get(\"image\", {}).get(\n",
    "    \"model_name\", \"openai/clip-vit-base-patch32\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ–¼ï¸  Image Embedding Configuration:\")\n",
    "print(f\"   Model: {IMAGE_MODEL_NAME}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Cache directory: {IMG_CACHE_DIR}\")\n",
    "\n",
    "# Load CLIP model and processor\n",
    "print(\"\\nâ³ Loading CLIP model...\")\n",
    "clip_model = CLIPModel.from_pretrained(IMAGE_MODEL_NAME).to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(IMAGE_MODEL_NAME)\n",
    "clip_model.eval()\n",
    "print(\"âœ… CLIP model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d089abbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Image processing utilities ready\n"
     ]
    }
   ],
   "source": [
    "# Image processing utilities\n",
    "\n",
    "def fetch_image(url: str, timeout: int = 15) -> Optional[Image.Image]:\n",
    "    \"\"\"\n",
    "    Download and cache an image from URL.\n",
    "    Returns RGB PIL Image or None if failed.\n",
    "    \"\"\"\n",
    "    url = ensure_string(url)\n",
    "    if not url:\n",
    "        return None\n",
    "    \n",
    "    # Create cache filename from URL hash\n",
    "    cache_filename = hashlib.sha256(url.encode()).hexdigest() + \".jpg\"\n",
    "    cache_path = IMG_CACHE_DIR / cache_filename\n",
    "    \n",
    "    try:\n",
    "        # Check cache first\n",
    "        if cache_path.exists() and cache_path.stat().st_size > 0:\n",
    "            img = Image.open(cache_path)\n",
    "            return img.convert(\"RGB\")\n",
    "        \n",
    "        # Download image\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to cache\n",
    "        with open(cache_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Load and return\n",
    "        img = Image.open(cache_path)\n",
    "        return img.convert(\"RGB\")\n",
    "    \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_images_batch(urls: List[str], batch_size: int = 32) -> Tuple[np.ndarray, List[bool]]:\n",
    "    \"\"\"\n",
    "    Generate CLIP embeddings for a list of image URLs.\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: numpy array of embeddings (only for successful images)\n",
    "        success_mask: boolean list indicating which URLs were successfully processed\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    success_mask = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(urls), batch_size):\n",
    "        batch_urls = urls[i:i + batch_size]\n",
    "        batch_images = []\n",
    "        batch_success = []\n",
    "        \n",
    "        # Fetch images for this batch\n",
    "        for url in batch_urls:\n",
    "            img = fetch_image(url)\n",
    "            if img is not None:\n",
    "                batch_images.append(img)\n",
    "                batch_success.append(True)\n",
    "            else:\n",
    "                batch_success.append(False)\n",
    "        \n",
    "        # Process valid images\n",
    "        if batch_images:\n",
    "            inputs = clip_processor(images=batch_images, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            \n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "            features_np = features.cpu().numpy().astype(np.float32)\n",
    "            \n",
    "            embeddings_list.append(features_np)\n",
    "        \n",
    "        success_mask.extend(batch_success)\n",
    "    \n",
    "    if not embeddings_list:\n",
    "        return np.zeros((0, 512), dtype=np.float32), success_mask\n",
    "    \n",
    "    return np.vstack(embeddings_list), success_mask\n",
    "\n",
    "\n",
    "print(\"âœ… Image processing utilities ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfbb32d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â³ Generating image embeddings for 10,000 stratified products...\n",
      "âœ… Image embeddings generated:\n",
      "   Shape: (9982, 512)\n",
      "   Successfully embedded: 9982 / 10000\n",
      "   Success rate: 99.8%\n"
     ]
    }
   ],
   "source": [
    "# Generate image embeddings for stratified sample only\n",
    "if \"image_url\" not in work_df.columns or len(image_sample_indices) == 0:\n",
    "    print(\"âš ï¸  No images to embed\")\n",
    "    image_embeddings = np.zeros((0, 512), dtype=np.float32)\n",
    "    image_success_mask = [False] * len(work_df)\n",
    "    image_product_indices = []\n",
    "else:\n",
    "    print(f\"\\nâ³ Generating image embeddings for {len(image_sample_indices):,} stratified products...\")\n",
    "    \n",
    "    # Get URLs only for sampled products\n",
    "    sampled_urls = [work_df.loc[idx, \"image_url\"] for idx in image_sample_indices]\n",
    "    \n",
    "    image_embeddings, success_mask_sampled = embed_images_batch(sampled_urls, batch_size=32)\n",
    "    \n",
    "    # Map success mask back to full work_df indices\n",
    "    image_success_mask = [False] * len(work_df)\n",
    "    image_product_indices = []\n",
    "    \n",
    "    for i, idx in enumerate(image_sample_indices):\n",
    "        if success_mask_sampled[i]:\n",
    "            image_success_mask[idx] = True\n",
    "            image_product_indices.append(idx)\n",
    "    \n",
    "    print(f\"âœ… Image embeddings generated:\")\n",
    "    print(f\"   Shape: {image_embeddings.shape}\")\n",
    "    print(f\"   Successfully embedded: {len(image_product_indices)} / {len(image_sample_indices)}\")\n",
    "    print(f\"   Success rate: {len(image_product_indices) / len(image_sample_indices) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba970cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChromaDB available\n",
      "\n",
      "ğŸ—„ï¸  Initializing ChromaDB at: /home/roshan/Aria/mlops/outputs/vector_db\n",
      "{'text_model': 'sentence-transformers/all-MiniLM-L12-v2', 'image_model': 'openai/clip-vit-base-patch32', 'random_seed': 42, 'device': 'cuda'}\n",
      "âœ… New text collection created\n",
      "âœ… New image collection created\n"
     ]
    }
   ],
   "source": [
    "# Import vector database\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "print(\"âœ… ChromaDB available\")\n",
    "\n",
    "\n",
    "# Initialize persistent ChromaDB client\n",
    "print(f\"\\nğŸ—„ï¸  Initializing ChromaDB at: {VDB_DIR}\")\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=str(VDB_DIR),\n",
    "    settings=Settings(anonymized_telemetry=False)\n",
    ")\n",
    "\n",
    "# Metadata for version tracking\n",
    "run_metadata = {\n",
    "    \"text_model\": TEXT_MODEL_NAME,\n",
    "    \"image_model\": IMAGE_MODEL_NAME,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"device\": DEVICE,\n",
    "}\n",
    "print(run_metadata)\n",
    "# Get or create collections\n",
    "try:\n",
    "    col_text = chroma_client.get_collection(\"products_text\")\n",
    "    print(\"âœ… Existing text collection loaded\")\n",
    "except:\n",
    "    col_text = chroma_client.create_collection(\"products_text\", metadata=run_metadata)\n",
    "    print(\"âœ… New text collection created\")\n",
    "\n",
    "try:\n",
    "    col_image = chroma_client.get_collection(\"products_image\")\n",
    "    print(\"âœ… Existing image collection loaded\")\n",
    "except:\n",
    "    col_image = chroma_client.create_collection(\"products_image\", metadata=run_metadata)\n",
    "    print(\"âœ… New image collection created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75fd42fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â³ Storing text embeddings in ChromaDB...\n",
      "   Progress: 5,461 / 1,022,296 text embeddings stored\n",
      "   Progress: 10,922 / 1,022,296 text embeddings stored\n",
      "   Progress: 16,383 / 1,022,296 text embeddings stored\n",
      "   Progress: 21,844 / 1,022,296 text embeddings stored\n",
      "   Progress: 27,305 / 1,022,296 text embeddings stored\n",
      "   Progress: 32,766 / 1,022,296 text embeddings stored\n",
      "   Progress: 38,227 / 1,022,296 text embeddings stored\n",
      "   Progress: 43,688 / 1,022,296 text embeddings stored\n",
      "   Progress: 49,149 / 1,022,296 text embeddings stored\n",
      "   Progress: 54,610 / 1,022,296 text embeddings stored\n",
      "   Progress: 60,071 / 1,022,296 text embeddings stored\n",
      "   Progress: 65,532 / 1,022,296 text embeddings stored\n",
      "   Progress: 70,993 / 1,022,296 text embeddings stored\n",
      "   Progress: 76,454 / 1,022,296 text embeddings stored\n",
      "   Progress: 81,915 / 1,022,296 text embeddings stored\n",
      "   Progress: 87,376 / 1,022,296 text embeddings stored\n",
      "   Progress: 92,837 / 1,022,296 text embeddings stored\n",
      "   Progress: 98,298 / 1,022,296 text embeddings stored\n",
      "   Progress: 103,759 / 1,022,296 text embeddings stored\n",
      "   Progress: 109,220 / 1,022,296 text embeddings stored\n",
      "   Progress: 114,681 / 1,022,296 text embeddings stored\n",
      "   Progress: 120,142 / 1,022,296 text embeddings stored\n",
      "   Progress: 125,603 / 1,022,296 text embeddings stored\n",
      "   Progress: 131,064 / 1,022,296 text embeddings stored\n",
      "   Progress: 136,525 / 1,022,296 text embeddings stored\n",
      "   Progress: 141,986 / 1,022,296 text embeddings stored\n",
      "   Progress: 147,447 / 1,022,296 text embeddings stored\n",
      "   Progress: 152,908 / 1,022,296 text embeddings stored\n",
      "   Progress: 158,369 / 1,022,296 text embeddings stored\n",
      "   Progress: 163,830 / 1,022,296 text embeddings stored\n",
      "   Progress: 169,291 / 1,022,296 text embeddings stored\n",
      "   Progress: 174,752 / 1,022,296 text embeddings stored\n",
      "   Progress: 180,213 / 1,022,296 text embeddings stored\n",
      "   Progress: 185,674 / 1,022,296 text embeddings stored\n",
      "   Progress: 191,135 / 1,022,296 text embeddings stored\n",
      "   Progress: 196,596 / 1,022,296 text embeddings stored\n",
      "   Progress: 202,057 / 1,022,296 text embeddings stored\n",
      "   Progress: 207,518 / 1,022,296 text embeddings stored\n",
      "   Progress: 212,979 / 1,022,296 text embeddings stored\n",
      "   Progress: 218,440 / 1,022,296 text embeddings stored\n",
      "   Progress: 223,901 / 1,022,296 text embeddings stored\n",
      "   Progress: 229,362 / 1,022,296 text embeddings stored\n",
      "   Progress: 234,823 / 1,022,296 text embeddings stored\n",
      "   Progress: 240,284 / 1,022,296 text embeddings stored\n",
      "   Progress: 245,745 / 1,022,296 text embeddings stored\n",
      "   Progress: 251,206 / 1,022,296 text embeddings stored\n",
      "   Progress: 256,667 / 1,022,296 text embeddings stored\n",
      "   Progress: 262,128 / 1,022,296 text embeddings stored\n",
      "   Progress: 267,589 / 1,022,296 text embeddings stored\n",
      "   Progress: 273,050 / 1,022,296 text embeddings stored\n",
      "   Progress: 278,511 / 1,022,296 text embeddings stored\n",
      "   Progress: 283,972 / 1,022,296 text embeddings stored\n",
      "   Progress: 289,433 / 1,022,296 text embeddings stored\n",
      "   Progress: 294,894 / 1,022,296 text embeddings stored\n",
      "   Progress: 300,355 / 1,022,296 text embeddings stored\n",
      "   Progress: 305,816 / 1,022,296 text embeddings stored\n",
      "   Progress: 311,277 / 1,022,296 text embeddings stored\n",
      "   Progress: 316,738 / 1,022,296 text embeddings stored\n",
      "   Progress: 322,199 / 1,022,296 text embeddings stored\n",
      "   Progress: 327,660 / 1,022,296 text embeddings stored\n",
      "   Progress: 333,121 / 1,022,296 text embeddings stored\n",
      "   Progress: 338,582 / 1,022,296 text embeddings stored\n",
      "   Progress: 344,043 / 1,022,296 text embeddings stored\n",
      "   Progress: 349,504 / 1,022,296 text embeddings stored\n",
      "   Progress: 354,965 / 1,022,296 text embeddings stored\n",
      "   Progress: 360,426 / 1,022,296 text embeddings stored\n",
      "   Progress: 365,887 / 1,022,296 text embeddings stored\n",
      "   Progress: 371,348 / 1,022,296 text embeddings stored\n",
      "   Progress: 376,809 / 1,022,296 text embeddings stored\n",
      "   Progress: 382,270 / 1,022,296 text embeddings stored\n",
      "   Progress: 387,731 / 1,022,296 text embeddings stored\n",
      "   Progress: 393,192 / 1,022,296 text embeddings stored\n",
      "   Progress: 398,653 / 1,022,296 text embeddings stored\n",
      "   Progress: 404,114 / 1,022,296 text embeddings stored\n",
      "   Progress: 409,575 / 1,022,296 text embeddings stored\n",
      "   Progress: 415,036 / 1,022,296 text embeddings stored\n",
      "   Progress: 420,497 / 1,022,296 text embeddings stored\n",
      "   Progress: 425,958 / 1,022,296 text embeddings stored\n",
      "   Progress: 431,419 / 1,022,296 text embeddings stored\n",
      "   Progress: 436,880 / 1,022,296 text embeddings stored\n",
      "   Progress: 442,341 / 1,022,296 text embeddings stored\n",
      "   Progress: 447,802 / 1,022,296 text embeddings stored\n",
      "   Progress: 453,263 / 1,022,296 text embeddings stored\n",
      "   Progress: 458,724 / 1,022,296 text embeddings stored\n",
      "   Progress: 464,185 / 1,022,296 text embeddings stored\n",
      "   Progress: 469,646 / 1,022,296 text embeddings stored\n",
      "   Progress: 475,107 / 1,022,296 text embeddings stored\n",
      "   Progress: 480,568 / 1,022,296 text embeddings stored\n",
      "   Progress: 486,029 / 1,022,296 text embeddings stored\n",
      "   Progress: 491,490 / 1,022,296 text embeddings stored\n",
      "   Progress: 496,951 / 1,022,296 text embeddings stored\n",
      "   Progress: 502,412 / 1,022,296 text embeddings stored\n",
      "   Progress: 507,873 / 1,022,296 text embeddings stored\n",
      "   Progress: 513,334 / 1,022,296 text embeddings stored\n",
      "   Progress: 518,795 / 1,022,296 text embeddings stored\n",
      "   Progress: 524,256 / 1,022,296 text embeddings stored\n",
      "   Progress: 529,717 / 1,022,296 text embeddings stored\n",
      "   Progress: 535,178 / 1,022,296 text embeddings stored\n",
      "   Progress: 540,639 / 1,022,296 text embeddings stored\n",
      "   Progress: 546,100 / 1,022,296 text embeddings stored\n",
      "   Progress: 551,561 / 1,022,296 text embeddings stored\n",
      "   Progress: 557,022 / 1,022,296 text embeddings stored\n",
      "   Progress: 562,483 / 1,022,296 text embeddings stored\n",
      "   Progress: 567,944 / 1,022,296 text embeddings stored\n",
      "   Progress: 573,405 / 1,022,296 text embeddings stored\n",
      "   Progress: 578,866 / 1,022,296 text embeddings stored\n",
      "   Progress: 584,327 / 1,022,296 text embeddings stored\n",
      "   Progress: 589,788 / 1,022,296 text embeddings stored\n",
      "   Progress: 595,249 / 1,022,296 text embeddings stored\n",
      "   Progress: 600,710 / 1,022,296 text embeddings stored\n",
      "   Progress: 606,171 / 1,022,296 text embeddings stored\n",
      "   Progress: 611,632 / 1,022,296 text embeddings stored\n",
      "   Progress: 617,093 / 1,022,296 text embeddings stored\n",
      "   Progress: 622,554 / 1,022,296 text embeddings stored\n",
      "   Progress: 628,015 / 1,022,296 text embeddings stored\n",
      "   Progress: 633,476 / 1,022,296 text embeddings stored\n",
      "   Progress: 638,937 / 1,022,296 text embeddings stored\n",
      "   Progress: 644,398 / 1,022,296 text embeddings stored\n",
      "   Progress: 649,859 / 1,022,296 text embeddings stored\n",
      "   Progress: 655,320 / 1,022,296 text embeddings stored\n",
      "   Progress: 660,781 / 1,022,296 text embeddings stored\n",
      "   Progress: 666,242 / 1,022,296 text embeddings stored\n",
      "   Progress: 671,703 / 1,022,296 text embeddings stored\n",
      "   Progress: 677,164 / 1,022,296 text embeddings stored\n",
      "   Progress: 682,625 / 1,022,296 text embeddings stored\n",
      "   Progress: 688,086 / 1,022,296 text embeddings stored\n",
      "   Progress: 693,547 / 1,022,296 text embeddings stored\n",
      "   Progress: 699,008 / 1,022,296 text embeddings stored\n",
      "   Progress: 704,469 / 1,022,296 text embeddings stored\n",
      "   Progress: 709,930 / 1,022,296 text embeddings stored\n",
      "   Progress: 715,391 / 1,022,296 text embeddings stored\n",
      "   Progress: 720,852 / 1,022,296 text embeddings stored\n",
      "   Progress: 726,313 / 1,022,296 text embeddings stored\n",
      "   Progress: 731,774 / 1,022,296 text embeddings stored\n",
      "   Progress: 737,235 / 1,022,296 text embeddings stored\n",
      "   Progress: 742,696 / 1,022,296 text embeddings stored\n",
      "   Progress: 748,157 / 1,022,296 text embeddings stored\n",
      "   Progress: 753,618 / 1,022,296 text embeddings stored\n",
      "   Progress: 759,079 / 1,022,296 text embeddings stored\n",
      "   Progress: 764,540 / 1,022,296 text embeddings stored\n",
      "   Progress: 770,001 / 1,022,296 text embeddings stored\n",
      "   Progress: 775,462 / 1,022,296 text embeddings stored\n",
      "   Progress: 780,923 / 1,022,296 text embeddings stored\n",
      "   Progress: 786,384 / 1,022,296 text embeddings stored\n",
      "   Progress: 791,845 / 1,022,296 text embeddings stored\n",
      "   Progress: 797,306 / 1,022,296 text embeddings stored\n",
      "   Progress: 802,767 / 1,022,296 text embeddings stored\n",
      "   Progress: 808,228 / 1,022,296 text embeddings stored\n",
      "   Progress: 813,689 / 1,022,296 text embeddings stored\n",
      "   Progress: 819,150 / 1,022,296 text embeddings stored\n",
      "   Progress: 824,611 / 1,022,296 text embeddings stored\n",
      "   Progress: 830,072 / 1,022,296 text embeddings stored\n",
      "   Progress: 835,533 / 1,022,296 text embeddings stored\n",
      "   Progress: 840,994 / 1,022,296 text embeddings stored\n",
      "   Progress: 846,455 / 1,022,296 text embeddings stored\n",
      "   Progress: 851,916 / 1,022,296 text embeddings stored\n",
      "   Progress: 857,377 / 1,022,296 text embeddings stored\n",
      "   Progress: 862,838 / 1,022,296 text embeddings stored\n",
      "   Progress: 868,299 / 1,022,296 text embeddings stored\n",
      "   Progress: 873,760 / 1,022,296 text embeddings stored\n",
      "   Progress: 879,221 / 1,022,296 text embeddings stored\n",
      "   Progress: 884,682 / 1,022,296 text embeddings stored\n",
      "   Progress: 890,143 / 1,022,296 text embeddings stored\n",
      "   Progress: 895,604 / 1,022,296 text embeddings stored\n",
      "   Progress: 901,065 / 1,022,296 text embeddings stored\n",
      "   Progress: 906,526 / 1,022,296 text embeddings stored\n",
      "   Progress: 911,987 / 1,022,296 text embeddings stored\n",
      "   Progress: 917,448 / 1,022,296 text embeddings stored\n",
      "   Progress: 922,909 / 1,022,296 text embeddings stored\n",
      "   Progress: 928,370 / 1,022,296 text embeddings stored\n",
      "   Progress: 933,831 / 1,022,296 text embeddings stored\n",
      "   Progress: 939,292 / 1,022,296 text embeddings stored\n",
      "   Progress: 944,753 / 1,022,296 text embeddings stored\n",
      "   Progress: 950,214 / 1,022,296 text embeddings stored\n",
      "   Progress: 955,675 / 1,022,296 text embeddings stored\n",
      "   Progress: 961,136 / 1,022,296 text embeddings stored\n",
      "   Progress: 966,597 / 1,022,296 text embeddings stored\n",
      "   Progress: 972,058 / 1,022,296 text embeddings stored\n",
      "   Progress: 977,519 / 1,022,296 text embeddings stored\n",
      "   Progress: 982,980 / 1,022,296 text embeddings stored\n",
      "   Progress: 988,441 / 1,022,296 text embeddings stored\n",
      "   Progress: 993,902 / 1,022,296 text embeddings stored\n",
      "   Progress: 999,363 / 1,022,296 text embeddings stored\n",
      "   Progress: 1,004,824 / 1,022,296 text embeddings stored\n",
      "   Progress: 1,010,285 / 1,022,296 text embeddings stored\n",
      "   Progress: 1,015,746 / 1,022,296 text embeddings stored\n",
      "   Progress: 1,021,207 / 1,022,296 text embeddings stored\n",
      "   Progress: 1,022,296 / 1,022,296 text embeddings stored\n",
      "âœ… Stored 1,022,296 text embeddings in ChromaDB\n"
     ]
    }
   ],
   "source": [
    "# Store text embeddings in ChromaDB\n",
    "print(\"\\nâ³ Storing text embeddings in ChromaDB...\")\n",
    "\n",
    "ids = work_df[\"random_key\"].astype(str).tolist()\n",
    "documents = work_df[\"product_text\"].astype(str).tolist()\n",
    "\n",
    "metadatas = []\n",
    "for _, row in work_df.iterrows():\n",
    "    metadatas.append({\n",
    "        \"modality\": \"text\",\n",
    "        \"persian_name\": ensure_string(row.get(\"persian_name\", \"\")),\n",
    "        \"brand_title\": ensure_string(row.get(\"brand_title\", \"\")),\n",
    "        \"category_title\": ensure_string(row.get(\"category_title\", \"\")),\n",
    "    })\n",
    "\n",
    "# Batch upsert to avoid exceeding ChromaDB's max batch size\n",
    "BATCH_SIZE = 5461\n",
    "total_stored = 0\n",
    "\n",
    "for i in range(0, len(ids), BATCH_SIZE):\n",
    "    batch_ids = ids[i:i + BATCH_SIZE]\n",
    "    batch_embeddings = text_embeddings[i:i + BATCH_SIZE].tolist()\n",
    "    batch_documents = documents[i:i + BATCH_SIZE]\n",
    "    batch_metadatas = metadatas[i:i + BATCH_SIZE]\n",
    "    \n",
    "    col_text.upsert(\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_embeddings,\n",
    "        documents=batch_documents,\n",
    "        metadatas=batch_metadatas\n",
    "    )\n",
    "    \n",
    "    total_stored += len(batch_ids)\n",
    "    print(f\"   Progress: {total_stored:,} / {len(ids):,} text embeddings stored\")\n",
    "\n",
    "print(f\"âœ… Stored {len(ids):,} text embeddings in ChromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ad8951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â³ Storing image embeddings in ChromaDB...\n",
      "   Progress: 1,000 / 9,982 image embeddings stored\n",
      "   Progress: 2,000 / 9,982 image embeddings stored\n",
      "   Progress: 3,000 / 9,982 image embeddings stored\n",
      "   Progress: 4,000 / 9,982 image embeddings stored\n",
      "   Progress: 5,000 / 9,982 image embeddings stored\n",
      "   Progress: 6,000 / 9,982 image embeddings stored\n",
      "   Progress: 7,000 / 9,982 image embeddings stored\n",
      "   Progress: 8,000 / 9,982 image embeddings stored\n",
      "   Progress: 9,000 / 9,982 image embeddings stored\n",
      "   Progress: 9,982 / 9,982 image embeddings stored\n",
      "âœ… Stored 9,982 image embeddings in ChromaDB\n"
     ]
    }
   ],
   "source": [
    "# Store image embeddings in ChromaDB\n",
    "if \"image_url\" in work_df.columns and len(image_product_indices) > 0:\n",
    "    print(\"\\nâ³ Storing image embeddings in ChromaDB...\")\n",
    "    \n",
    "    # Get IDs and URLs for successfully embedded images\n",
    "    ids = [work_df.loc[idx, \"random_key\"] for idx in image_product_indices]\n",
    "    urls = [work_df.loc[idx, \"image_url\"] for idx in image_product_indices]\n",
    "    \n",
    "    metadatas = [{\"modality\": \"image\", \"image_url\": ensure_string(url)} for url in urls]\n",
    "    \n",
    "    # Batch upsert to avoid exceeding ChromaDB's max batch size\n",
    "    BATCH_SIZE = 1000\n",
    "    total_stored = 0\n",
    "    \n",
    "    for i in range(0, len(ids), BATCH_SIZE):\n",
    "        batch_ids = [str(id_) for id_ in ids[i:i + BATCH_SIZE]]\n",
    "        batch_embeddings = image_embeddings[i:i + BATCH_SIZE].tolist()\n",
    "        batch_metadatas = metadatas[i:i + BATCH_SIZE]\n",
    "        \n",
    "        col_image.upsert(\n",
    "            ids=batch_ids,\n",
    "            embeddings=batch_embeddings,\n",
    "            metadatas=batch_metadatas\n",
    "        )\n",
    "        \n",
    "        total_stored += len(batch_ids)\n",
    "        print(f\"   Progress: {total_stored:,} / {len(ids):,} image embeddings stored\")\n",
    "    \n",
    "    print(f\"âœ… Stored {len(ids):,} image embeddings in ChromaDB\")\n",
    "else:\n",
    "    print(\"âš ï¸  No images to store in ChromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af6b5a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Collecting metadata...\n",
      "âœ… Metadata saved to: /home/roshan/Aria/mlops/outputs/embeddings/feature_engineering_metadata.json\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING METADATA\n",
      "============================================================\n",
      "{\n",
      "  \"created_at\": \"2026-01-04T14:29:38.606704\",\n",
      "  \"random_seed\": 42,\n",
      "  \"models\": {\n",
      "    \"text_model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n",
      "    \"image_model\": \"openai/clip-vit-base-patch32\"\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"total_products\": 1022296,\n",
      "    \"sample_size\": null,\n",
      "    \"processed_products\": 1022296,\n",
      "    \"text_embeddings\": 1022296,\n",
      "    \"max_images_target\": 10000,\n",
      "    \"images_sampled\": 10000,\n",
      "    \"image_embeddings\": 9982\n",
      "  },\n",
      "  \"configuration\": {\n",
      "    \"text_batch_size\": 64,\n",
      "    \"device\": \"cuda\",\n",
      "    \"stratified_sampling\": true\n",
      "  },\n",
      "  \"package_versions\": {\n",
      "    \"torch\": \"2.8.0+cu128\",\n",
      "    \"transformers\": \"4.56.1\",\n",
      "    \"sentence_transformers\": \"4.0.2\",\n",
      "    \"chromadb\": \"1.4.0\",\n",
      "    \"numpy\": \"2.2.6\",\n",
      "    \"pandas\": \"2.3.2\",\n",
      "    \"sklearn\": \"1.6.1\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Collect package versions\n",
    "print(\"\\nğŸ“ Collecting metadata...\")\n",
    "\n",
    "package_versions = {}\n",
    "for package_name in [\"torch\", \"transformers\", \"sentence_transformers\", \"chromadb\", \"numpy\", \"pandas\", \"sklearn\"]:\n",
    "    try:\n",
    "        module = __import__(package_name)\n",
    "        package_versions[package_name] = module.__version__\n",
    "    except:\n",
    "        package_versions[package_name] = \"not installed\"\n",
    "\n",
    "# Create comprehensive metadata\n",
    "metadata = {\n",
    "    \"created_at\": pd.Timestamp.now().isoformat(),\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"models\": {\n",
    "        \"text_model\": TEXT_MODEL_NAME,\n",
    "        \"image_model\": IMAGE_MODEL_NAME,\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"total_products\": len(base_products),\n",
    "        \"sample_size\": SAMPLE_SIZE,\n",
    "        \"processed_products\": len(work_df),\n",
    "        \"text_embeddings\": len(text_embeddings),\n",
    "        \"max_images_target\": MAX_IMAGES if 'MAX_IMAGES' in locals() else None,\n",
    "        \"images_sampled\": len(image_sample_indices) if 'image_sample_indices' in locals() else 0,\n",
    "        \"image_embeddings\": len(image_product_indices) if 'image_product_indices' in locals() else 0,\n",
    "    },\n",
    "    \"configuration\": {\n",
    "        \"text_batch_size\": TEXT_BATCH_SIZE,\n",
    "        \"device\": DEVICE,\n",
    "        \"stratified_sampling\": True,\n",
    "    },\n",
    "    \"package_versions\": package_versions,\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = EMB_DIR / \"feature_engineering_metadata.json\"\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Metadata saved to: {metadata_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING METADATA\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(metadata, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d11fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RETRIEVAL VALIDATION TEST\n",
      "============================================================\n",
      "\n",
      "ğŸ” Query: ÛŒØ®Ú†Ø§Ù„\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Product ID: oximkp\n",
      "   Distance: 0.2321\n",
      "   Brand: \n",
      "   Category: ÛŒØ®Ú†Ø§Ù„\n",
      "   Text: ÛŒØ®Ú†Ø§Ù„ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ÛŒ Û·Û° Ø³Ø§Ù†Øª Ø¬Ù†Ø³ Ø¨Ø¯Ù†Ù‡ Ú¯Ø§Ù„ÙˆØ§Ù†ÛŒØ²Ù‡ | Ø¯Ø³ØªÙ‡: ÛŒØ®Ú†Ø§Ù„...\n",
      "\n",
      "2. Product ID: njpspn\n",
      "   Distance: 0.2394\n",
      "   Brand: \n",
      "   Category: ÛŒØ®Ú†Ø§Ù„\n",
      "   Text: ÛŒØ®Ú†Ø§Ù„ Ù…ÛŒÙ†ÛŒ Ø¨Ø§Ø± Ø¨ÙˆØ´ Ù…Ø¯Ù„ KUW21AHG0 | Ø¯Ø³ØªÙ‡: ÛŒØ®Ú†Ø§Ù„...\n",
      "\n",
      "3. Product ID: blvked\n",
      "   Distance: 0.2402\n",
      "   Brand: \n",
      "   Category: ÛŒØ®Ú†Ø§Ù„\n",
      "   Text: ÛŒØ®Ú†Ø§Ù„ Û±Û° ÙÙˆØª Ú©Ù„Ø§Ø³ÛŒÚ© Ø§Ù…Ø±Ø³Ø§ | Ø¯Ø³ØªÙ‡: ÛŒØ®Ú†Ø§Ù„...\n",
      "\n",
      "ğŸ” Query: ÛŒØ®Ú†Ø§Ù„ ÙØ±ÛŒØ²Ø± Ø·Ø±Ø­ Ø³Ø§ÛŒØ¯ Ù¾Ø§Ú©Ø´ÙˆÙ…Ø§\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Product ID: ijsjpo\n",
      "   Distance: 0.2581\n",
      "   Brand: \n",
      "   Category: Ù„ÙˆØ§Ø²Ù… ÛŒØ¯Ú©ÛŒ Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡\n",
      "   Text: Ù¾Ø§Ø±Ú† Ø·Ù„Ù‚ÛŒ Ø´Ø§Ù‡ÛŒÙ† | Ø¯Ø³ØªÙ‡: Ù„ÙˆØ§Ø²Ù… ÛŒØ¯Ú©ÛŒ Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡...\n",
      "\n",
      "2. Product ID: zoentt\n",
      "   Distance: 0.2855\n",
      "   Brand: \n",
      "   Category: Ù„ÙˆØ§Ø²Ù… ÛŒØ¯Ú©ÛŒ Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡\n",
      "   Text: Ø¯Ø³ØªÙ‡ Ø¸Ø±ÙˆÙ Ù¾ÛŒÚ† Ø·Ù„Ø§ÛŒÛŒ | Ø¯Ø³ØªÙ‡: Ù„ÙˆØ§Ø²Ù… ÛŒØ¯Ú©ÛŒ Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡...\n",
      "\n",
      "3. Product ID: bxrhon\n",
      "   Distance: 0.2927\n",
      "   Brand: \n",
      "   Category: Ù„ÙˆØ§Ø²Ù… ÛŒØ¯Ú©ÛŒ Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡\n",
      "   Text: Ù¾Ø§Ø±Ú† Ø·Ù„Ù‚ÛŒ Ù…Ø®Ù„ÙˆØ· Ú©Ù† Ø¨Ø±Ù†Ø¯ Ú©Ù†ÙˆÙˆØ¯ | Ø¯Ø³ØªÙ‡: Ù„ÙˆØ§Ø²Ù… ÛŒØ¯Ú©ÛŒ Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡...\n",
      "\n",
      "ğŸ” Query: Ø¨Ø·Ø±ÛŒ Ø´ÛŒØ´Ù‡ Ø§ÛŒ\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Product ID: ktflmc\n",
      "   Distance: 0.1759\n",
      "   Brand: \n",
      "   Category: Ø¨Ø·Ø±ÛŒ Ù†ÙˆØ´ÛŒØ¯Ù†ÛŒ\n",
      "   Text: Ø¨Ø·Ø±ÛŒ Ø´ÛŒØ´Ù‡ Ø§ÛŒ Ú©Ø¯ Û·Û¶Û´ | Ø¯Ø³ØªÙ‡: Ø¨Ø·Ø±ÛŒ Ù†ÙˆØ´ÛŒØ¯Ù†ÛŒ...\n",
      "\n",
      "2. Product ID: aiibkh\n",
      "   Distance: 0.1815\n",
      "   Brand: \n",
      "   Category: Ø¨Ø·Ø±ÛŒ Ù†ÙˆØ´ÛŒØ¯Ù†ÛŒ\n",
      "   Text: Ø¨Ø·Ø±ÛŒ Ø´ÛŒØ´Ù‡ Ø§ÛŒ Ú©Ø´ØªÛŒ | Ø¯Ø³ØªÙ‡: Ø¨Ø·Ø±ÛŒ Ù†ÙˆØ´ÛŒØ¯Ù†ÛŒ...\n",
      "\n",
      "3. Product ID: rinyur\n",
      "   Distance: 0.1878\n",
      "   Brand: \n",
      "   Category: Ø¨Ø·Ø±ÛŒ Ù†ÙˆØ´ÛŒØ¯Ù†ÛŒ\n",
      "   Text: Ø¨Ø·Ø±ÛŒ Ø´ÛŒØ´Ù‡ Ø§ÛŒ Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ | Ø¯Ø³ØªÙ‡: Ø¨Ø·Ø±ÛŒ Ù†ÙˆØ´ÛŒØ¯Ù†ÛŒ...\n",
      "\n",
      "============================================================\n",
      "âœ… Feature Engineering Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test text retrieval with sample queries\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRIEVAL VALIDATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    \"\"\"Embed a user query (MiniLM doesn't use prefixes).\"\"\"\n",
    "    normalized_query = normalize_persian_text(query)\n",
    "    embedding = text_model.encode([normalized_query], normalize_embeddings=True)\n",
    "    return np.asarray(embedding, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Test with sample Persian queries\n",
    "test_queries = [\n",
    "    \"ÛŒØ®Ú†Ø§Ù„\",\n",
    "    \"ÛŒØ®Ú†Ø§Ù„ ÙØ±ÛŒØ²Ø± Ø·Ø±Ø­ Ø³Ø§ÛŒØ¯ Ù¾Ø§Ú©Ø´ÙˆÙ…Ø§\",\n",
    "    \"Ø¨Ø·Ø±ÛŒ Ø´ÛŒØ´Ù‡ Ø§ÛŒ\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nğŸ” Query: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    query_embedding = embed_query(query)\n",
    "    \n",
    "    results = col_text.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=3,\n",
    "        include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    for i, (doc_id, distance, doc, metadata) in enumerate(zip(\n",
    "        results[\"ids\"][0],\n",
    "        results[\"distances\"][0],\n",
    "        results[\"documents\"][0],\n",
    "        results[\"metadatas\"][0]\n",
    "    )):\n",
    "        print(f\"\\n{i+1}. Product ID: {doc_id}\")\n",
    "        print(f\"   Distance: {distance:.4f}\")\n",
    "        print(f\"   Brand: {metadata.get('brand_title', 'N/A')}\")\n",
    "        print(f\"   Category: {metadata.get('category_title', 'N/A')}\")\n",
    "        print(f\"   Text: {doc[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Feature Engineering Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
