{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2b824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "Pandas version: 2.3.2\n",
      "NumPy version: 2.2.6\n",
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970e528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded from config.yaml\n",
      "\n",
      "üìÅ Data paths:\n",
      "   Raw: Data/\n",
      "   Processed: Data/processed/\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "CONFIG_PATH = Path('../configs/config.yaml')\n",
    "\n",
    "if CONFIG_PATH.exists():\n",
    "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"‚úÖ Configuration loaded from config.yaml\")\n",
    "else:\n",
    "    # Default configuration\n",
    "    config = {\n",
    "        'data': {\n",
    "            'raw_path': '../Data/',\n",
    "            'processed_path': '../Data/processed/',\n",
    "        },\n",
    "        'cleaning': {\n",
    "            'placeholder_tokens': ['none', 'null', 'nan', 'n/a', 'na', '-', '--', 'unknown', '?'],\n",
    "            'price_min': 1000,  # Minimum valid price (1,000 Toman)\n",
    "            'price_max': 1_000_000_000_000,  # Maximum valid price (1 Trillion Toman)\n",
    "            'remove_zero_prices': True,\n",
    "        },\n",
    "        'random_seed': RANDOM_SEED\n",
    "    }\n",
    "    print(\"‚ö†Ô∏è  Using default configuration\")\n",
    "\n",
    "# Create processed data directory\n",
    "PROCESSED_PATH = Path(config['data']['processed_path'])\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Data paths:\")\n",
    "print(f\"   Raw: {config['data']['raw_path']}\")\n",
    "print(f\"   Processed: {config['data']['processed_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c806bcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ All datasets loaded successfully!\n",
      "--------------------------------------------------------------------------------\n",
      "base_products       :  1,022,298 rows √ó   8 columns\n",
      "members             :  1,948,665 rows √ó   4 columns\n",
      "searches            :    588,347 rows √ó   9 columns\n",
      "base_views          :    199,916 rows √ó   4 columns\n",
      "final_clicks        :     17,371 rows √ó   4 columns\n",
      "shops               :     23,342 rows √ó   4 columns\n",
      "categories          :        746 rows √ó   3 columns\n",
      "brands              :      2,025 rows √ó   2 columns\n",
      "cities              :        651 rows √ó   2 columns\n"
     ]
    }
   ],
   "source": [
    "# Define data path\n",
    "DATA_PATH = Path(config['data']['raw_path'])\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load all tables\n",
    "base_products = pd.read_parquet('../Data/base_products.parquet')\n",
    "members = pd.read_parquet('../Data/members.parquet')\n",
    "searches = pd.read_parquet('../Data/searches.parquet')\n",
    "base_views = pd.read_parquet('../Data/base_views.parquet')\n",
    "final_clicks = pd.read_parquet('../Data/final_clicks.parquet')\n",
    "shops = pd.read_parquet('../Data/shops.parquet')\n",
    "categories = pd.read_parquet('../Data/categories.parquet')\n",
    "brands = pd.read_parquet('../Data/brands.parquet')\n",
    "cities = pd.read_parquet('../Data/cities.parquet')\n",
    "\n",
    "print(\"‚úÖ All datasets loaded successfully!\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Store original shapes for comparison later\n",
    "original_shapes = {\n",
    "    'base_products': base_products.shape,\n",
    "    'members': members.shape,\n",
    "    'searches': searches.shape,\n",
    "    'base_views': base_views.shape,\n",
    "    'final_clicks': final_clicks.shape,\n",
    "    'shops': shops.shape,\n",
    "    'categories': categories.shape,\n",
    "    'brands': brands.shape,\n",
    "    'cities': cities.shape\n",
    "}\n",
    "\n",
    "for name, shape in original_shapes.items():\n",
    "    print(f\"{name:20s}: {shape[0]:>10,} rows √ó {shape[1]:>3} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f5a4ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text cleaning functions defined:\n",
      "   - clean_text_field(): General text cleaning\n",
      "   - normalize_persian_text(): Persian-specific normalization\n",
      "   - normalize_english_text(): English-specific normalization\n"
     ]
    }
   ],
   "source": [
    "def clean_text_field(series, placeholder_tokens=None, convert_to_none=True):\n",
    "    \"\"\"\n",
    "    Clean and normalize text fields.\n",
    "    \n",
    "    Args:\n",
    "        series: pandas Series to clean\n",
    "        placeholder_tokens: list of strings to treat as missing (e.g., ['null', 'none', 'n/a'])\n",
    "        convert_to_none: if True, convert empty/placeholder values to None; otherwise keep as empty string\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned pandas Series\n",
    "    \"\"\"\n",
    "    if placeholder_tokens is None:\n",
    "        placeholder_tokens = config['cleaning']['placeholder_tokens']\n",
    "    \n",
    "    # Convert to string type\n",
    "    cleaned = series.astype('string')\n",
    "    \n",
    "    # Strip whitespace\n",
    "    cleaned = cleaned.str.strip()\n",
    "    \n",
    "    # Remove multiple consecutive spaces\n",
    "    cleaned = cleaned.str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # Handle placeholders\n",
    "    placeholder_set = set([str(p).lower() for p in placeholder_tokens])\n",
    "    mask_placeholder = cleaned.str.lower().isin(placeholder_set)\n",
    "    \n",
    "    if convert_to_none:\n",
    "        # Convert empty strings and placeholders to None\n",
    "        cleaned = cleaned.where(\n",
    "            (cleaned.notna()) & (cleaned != '') & (~mask_placeholder),\n",
    "            None\n",
    "        )\n",
    "    else:\n",
    "        # Convert placeholders to empty string\n",
    "        cleaned = cleaned.where(~mask_placeholder, '')\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def normalize_persian_text(series):\n",
    "    \"\"\"\n",
    "    Normalize Persian/Arabic characters.\n",
    "    - Convert Arabic 'Ÿä' and 'ŸÉ' to Persian '€å' and '⁄©'\n",
    "    - Remove zero-width characters\n",
    "    - Normalize diacritics\n",
    "    \"\"\"\n",
    "    if series is None or len(series) == 0:\n",
    "        return series\n",
    "    \n",
    "    cleaned = series.astype('string')\n",
    "    \n",
    "    # Arabic to Persian character conversion\n",
    "    cleaned = cleaned.str.replace('Ÿä', '€å', regex=False)  # Arabic Yeh to Persian Yeh\n",
    "    cleaned = cleaned.str.replace('ŸÉ', '⁄©', regex=False)  # Arabic Kaf to Persian Kaf\n",
    "    \n",
    "    # Remove zero-width characters (ZWJ, ZWNJ, etc.)\n",
    "    # Keep ZWNJ (\\u200c) as it's important for Persian text\n",
    "    cleaned = cleaned.str.replace('[\\u200b\\u200d\\u200e\\u200f]', '', regex=True)\n",
    "    \n",
    "    # Remove diacritics (Ÿé Ÿè Ÿê Ÿã Ÿå Ÿç Ÿë Ÿí) for better text matching\n",
    "    cleaned = cleaned.str.replace('[\\u064B-\\u0652]', '', regex=True)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def normalize_english_text(series):\n",
    "    \"\"\"\n",
    "    Normalize English text.\n",
    "    - Convert to lowercase (optional)\n",
    "    - Remove special characters (optional)\n",
    "    \"\"\"\n",
    "    if series is None or len(series) == 0:\n",
    "        return series\n",
    "    \n",
    "    cleaned = series.astype('string')\n",
    "    \n",
    "    # Convert to lowercase for consistency (optional)\n",
    "    # cleaned = cleaned.str.lower()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "print(\"‚úÖ Text cleaning functions defined:\")\n",
    "print(\"   - clean_text_field(): General text cleaning\")\n",
    "print(\"   - normalize_persian_text(): Persian-specific normalization\")\n",
    "print(\"   - normalize_english_text(): English-specific normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0112216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLEANING BASE_PRODUCTS TABLE\n",
      "================================================================================\n",
      "\n",
      "üìä Original shape: (1022298, 8)\n",
      "\n",
      "1Ô∏è‚É£ Cleaning text fields...\n",
      "   ‚úÖ Removed english_name column (92% missing)\n",
      "   Persian name missing: 2 (0.000%)\n",
      "   Image URL missing: 3,341 (0.327%)\n",
      "\n",
      "2Ô∏è‚É£ Removing products without persian_name...\n",
      "   Removed 2 products without persian_name\n",
      "\n",
      "3Ô∏è‚É£ Adding flag for products without images...\n",
      "   Products with images: 1,018,955 (99.67%)\n",
      "   Products without images: 3,341 (0.33%)\n",
      "\n",
      "‚úÖ Base products cleaned: (1022296, 8)\n",
      "   Removed: 2 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CLEANING BASE_PRODUCTS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy to avoid modifying original\n",
    "base_products_clean = base_products.copy()\n",
    "\n",
    "print(f\"\\nüìä Original shape: {base_products_clean.shape}\")\n",
    "\n",
    "# Step 1: Clean text fields\n",
    "print(\"\\n1Ô∏è‚É£ Cleaning text fields...\")\n",
    "\n",
    "# Get placeholder tokens from config (with fallback)\n",
    "placeholder_tokens = config.get('cleaning', {}).get('placeholder_tokens', \n",
    "                                                     ['none', 'null', 'nan', 'n/a', 'na', '-', '--', 'unknown', '?'])\n",
    "\n",
    "# Persian name (critical field - must not be empty)\n",
    "base_products_clean['persian_name'] = clean_text_field(\n",
    "    base_products_clean['persian_name'], \n",
    "    placeholder_tokens=placeholder_tokens,\n",
    "    convert_to_none=True\n",
    ")\n",
    "base_products_clean['persian_name'] = normalize_persian_text(base_products_clean['persian_name'])\n",
    "\n",
    "# Image URL\n",
    "base_products_clean['image_url'] = clean_text_field(\n",
    "    base_products_clean['image_url'],\n",
    "    placeholder_tokens=placeholder_tokens,\n",
    "    convert_to_none=True\n",
    ")\n",
    "\n",
    "# Remove english_name column (92% missing - not useful)\n",
    "if 'english_name' in base_products_clean.columns:\n",
    "    base_products_clean = base_products_clean.drop(columns=['english_name'])\n",
    "    print(f\"   ‚úÖ Removed english_name column (92% missing)\")\n",
    "\n",
    "# Count missing values\n",
    "missing_persian = base_products_clean['persian_name'].isna().sum()\n",
    "missing_image = base_products_clean['image_url'].isna().sum()\n",
    "\n",
    "print(f\"   Persian name missing: {missing_persian:,} ({missing_persian/len(base_products_clean)*100:.3f}%)\")\n",
    "print(f\"   Image URL missing: {missing_image:,} ({missing_image/len(base_products_clean)*100:.3f}%)\")\n",
    "\n",
    "# Step 2: Remove products without persian_name (critical field)\n",
    "print(\"\\n2Ô∏è‚É£ Removing products without persian_name...\")\n",
    "before_removal = len(base_products_clean)\n",
    "base_products_clean = base_products_clean[base_products_clean['persian_name'].notna()].copy()\n",
    "removed = before_removal - len(base_products_clean)\n",
    "print(f\"   Removed {removed:,} products without persian_name\")\n",
    "\n",
    "# Step 3: Add flag for products without images\n",
    "print(\"\\n3Ô∏è‚É£ Adding flag for products without images...\")\n",
    "base_products_clean['has_image'] = base_products_clean['image_url'].notna()\n",
    "print(f\"   Products with images: {base_products_clean['has_image'].sum():,} ({base_products_clean['has_image'].sum()/len(base_products_clean)*100:.2f}%)\")\n",
    "print(f\"   Products without images: {(~base_products_clean['has_image']).sum():,} ({(~base_products_clean['has_image']).sum()/len(base_products_clean)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Base products cleaned: {base_products_clean.shape}\")\n",
    "print(f\"   Removed: {original_shapes['base_products'][0] - base_products_clean.shape[0]:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13d67f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARSING extra_features FIELD\n",
      "================================================================================\n",
      "\n",
      "üì¶ Parsing extra_features JSON...\n",
      "   Products with features: 1,022,296 (100.00%)\n",
      "   Products without features: 0 (0.00%)\n",
      "\n",
      "üìã Sample parsed features:\n",
      "\n",
      "   Product: ŸÅÿ±ÿ¥ 700ÿ¥ÿßŸÜŸá ÿßŸèŸæÿßŸÑ ŸÅ€åÿ±Ÿàÿ≤Ÿá ÿß€å\n",
      "   Features: {'meterage': ['12 m', '9 m', '6 m'], 'originality': '', 'number_combs': '700 ÿ¥ÿßŸÜŸá', 'stock_status': 'new', 'carpet_density': '2550', 'background_color': ['ŸÅ€åÿ±Ÿàÿ≤Ÿá ÿß€å']}\n",
      "\n",
      "   Product: ÿµŸÜÿØŸÑ€å ÿ™ÿßÿ® ŸÖÿØŸÑ chioco\n",
      "   Features: {'originality': '', 'stock_status': 'new'}\n",
      "\n",
      "   Product: ÿ®ÿ¥ŸÇÿßÿ® ŸÖÿ¨ŸÖŸàÿπŸá ⁄©€å⁄© €å⁄©Ÿæÿßÿ±⁄ÜŸáÿ≥ÿßÿ≤€å ÿ®ÿß ÿ≥€åÿ≥ÿ™ŸÖÿπÿßŸÖŸÑ 13 ŸÇÿ∑ÿπŸá 6 ÿ±ŸÜ⁄Ø 29.03.2025 English Home\n",
      "   Features: {'size2': '', 'originality': '', 'piece_count': '13 pieces', 'stock_status': 'new'}\n",
      "\n",
      "‚úÖ extra_features parsed successfully\n"
     ]
    }
   ],
   "source": [
    "def parse_extra_features(value):\n",
    "    \"\"\"\n",
    "    Parse extra_features field into a dictionary.\n",
    "    Handles various formats: dict, JSON string, empty, etc.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return {}\n",
    "    \n",
    "    # Already a dict\n",
    "    if isinstance(value, dict):\n",
    "        return value\n",
    "    \n",
    "    # Try to parse as JSON string\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if value == '' or value.lower() in ['null', 'none', '{}', '[]']:\n",
    "            return {}\n",
    "        try:\n",
    "            parsed = json.loads(value)\n",
    "            if isinstance(parsed, dict):\n",
    "                return parsed\n",
    "            elif isinstance(parsed, list):\n",
    "                # Convert list to dict if possible\n",
    "                return {'items': parsed}\n",
    "            else:\n",
    "                return {}\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            # If JSON parsing fails, return as single key-value\n",
    "            return {'raw_text': value}\n",
    "    \n",
    "    return {}\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARSING extra_features FIELD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Parse extra_features\n",
    "print(\"\\nüì¶ Parsing extra_features JSON...\")\n",
    "base_products_clean['extra_features_parsed'] = base_products_clean['extra_features'].apply(parse_extra_features)\n",
    "\n",
    "# Count non-empty features\n",
    "non_empty = base_products_clean['extra_features_parsed'].apply(lambda x: len(x) > 0).sum()\n",
    "print(f\"   Products with features: {non_empty:,} ({non_empty/len(base_products_clean)*100:.2f}%)\")\n",
    "print(f\"   Products without features: {len(base_products_clean) - non_empty:,} ({(len(base_products_clean) - non_empty)/len(base_products_clean)*100:.2f}%)\")\n",
    "\n",
    "# Sample parsed features\n",
    "print(\"\\nüìã Sample parsed features:\")\n",
    "sample_with_features = base_products_clean[base_products_clean['extra_features_parsed'].apply(lambda x: len(x) > 0)].head(3)\n",
    "for idx, row in sample_with_features.iterrows():\n",
    "    print(f\"\\n   Product: {row['persian_name']}\")\n",
    "    print(f\"   Features: {row['extra_features_parsed']}\")\n",
    "\n",
    "print(\"\\n‚úÖ extra_features parsed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "758298a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLEANING MEMBERS TABLE\n",
      "================================================================================\n",
      "\n",
      "üìä Original shape: (1948665, 4)\n",
      "\n",
      "1Ô∏è‚É£ Removing orphaned products (invalid base_random_key)...\n",
      "   Removed 145 orphaned products\n",
      "\n",
      "2Ô∏è‚É£ Cleaning price data...\n",
      "\n",
      "   Before cleaning:\n",
      "   - Total products: 1,948,520\n",
      "   - Mean price: 154,379,541,844 Toman\n",
      "   - Median price: 1,700,000 Toman\n",
      "   - Min price: 0 Toman\n",
      "   - Max price: 199,000,000,199,000,000 Toman\n",
      "\n",
      "   Problematic prices:\n",
      "   - Zero prices: 2 (0.00%)\n",
      "   - Negative prices: 0 (0.00%)\n",
      "   - Null prices: 0 (0.00%)\n",
      "   - Very low (<1,000): 691 (0.04%)\n",
      "   - Very high (>10,000,000,000): 86 (0.00%)\n",
      "\n",
      "   Removed 777 products with invalid prices\n",
      "\n",
      "   After cleaning:\n",
      "   - Total products: 1,947,743\n",
      "   - Mean price: 8,274,553 Toman\n",
      "   - Median price: 1,700,000 Toman\n",
      "   - Min price: 1,000 Toman\n",
      "   - Max price: 9,857,600,000 Toman\n",
      "\n",
      "3Ô∏è‚É£ Removing duplicate member products...\n",
      "   Removed 0 duplicate products\n",
      "\n",
      "‚úÖ Members table cleaned: (1947743, 4)\n",
      "   Total removed: 922 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CLEANING MEMBERS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy\n",
    "members_clean = members.copy()\n",
    "\n",
    "print(f\"\\nüìä Original shape: {members_clean.shape}\")\n",
    "\n",
    "# Step 1: Remove members with invalid base_random_key (orphaned products)\n",
    "print(\"\\n1Ô∏è‚É£ Removing orphaned products (invalid base_random_key)...\")\n",
    "valid_base_keys = set(base_products_clean['random_key'])\n",
    "before_orphan_removal = len(members_clean)\n",
    "members_clean = members_clean[members_clean['base_random_key'].isin(valid_base_keys)].copy()\n",
    "orphan_removed = before_orphan_removal - len(members_clean)\n",
    "print(f\"   Removed {orphan_removed:,} orphaned products\")\n",
    "\n",
    "# Step 2: Clean price data\n",
    "print(\"\\n2Ô∏è‚É£ Cleaning price data...\")\n",
    "\n",
    "# Get price thresholds from config (with fallback)\n",
    "price_min = config.get('cleaning', {}).get('price_min', 1000)\n",
    "price_max = config.get('cleaning', {}).get('price_max', 10_000_000_000)\n",
    "\n",
    "# Statistics before cleaning\n",
    "print(f\"\\n   Before cleaning:\")\n",
    "print(f\"   - Total products: {len(members_clean):,}\")\n",
    "print(f\"   - Mean price: {members_clean['price'].mean():,.0f} Toman\")\n",
    "print(f\"   - Median price: {members_clean['price'].median():,.0f} Toman\")\n",
    "print(f\"   - Min price: {members_clean['price'].min():,.0f} Toman\")\n",
    "print(f\"   - Max price: {members_clean['price'].max():,.0f} Toman\")\n",
    "\n",
    "# Count problematic prices\n",
    "zero_prices = (members_clean['price'] == 0).sum()\n",
    "negative_prices = (members_clean['price'] < 0).sum()\n",
    "null_prices = members_clean['price'].isna().sum()\n",
    "very_low = (members_clean['price'] < price_min).sum()\n",
    "very_high = (members_clean['price'] > price_max).sum()\n",
    "\n",
    "print(f\"\\n   Problematic prices:\")\n",
    "print(f\"   - Zero prices: {zero_prices:,} ({zero_prices/len(members_clean)*100:.2f}%)\")\n",
    "print(f\"   - Negative prices: {negative_prices:,} ({negative_prices/len(members_clean)*100:.2f}%)\")\n",
    "print(f\"   - Null prices: {null_prices:,} ({null_prices/len(members_clean)*100:.2f}%)\")\n",
    "print(f\"   - Very low (<{price_min:,}): {very_low:,} ({very_low/len(members_clean)*100:.2f}%)\")\n",
    "print(f\"   - Very high (>{price_max:,}): {very_high:,} ({very_high/len(members_clean)*100:.2f}%)\")\n",
    "\n",
    "# Remove invalid prices\n",
    "before_price_cleaning = len(members_clean)\n",
    "members_clean = members_clean[\n",
    "    (members_clean['price'].notna()) &\n",
    "    (members_clean['price'] >= price_min) &\n",
    "    (members_clean['price'] <= price_max)\n",
    "].copy()\n",
    "price_removed = before_price_cleaning - len(members_clean)\n",
    "print(f\"\\n   Removed {price_removed:,} products with invalid prices\")\n",
    "\n",
    "# Statistics after cleaning\n",
    "print(f\"\\n   After cleaning:\")\n",
    "print(f\"   - Total products: {len(members_clean):,}\")\n",
    "print(f\"   - Mean price: {members_clean['price'].mean():,.0f} Toman\")\n",
    "print(f\"   - Median price: {members_clean['price'].median():,.0f} Toman\")\n",
    "print(f\"   - Min price: {members_clean['price'].min():,.0f} Toman\")\n",
    "print(f\"   - Max price: {members_clean['price'].max():,.0f} Toman\")\n",
    "\n",
    "# Step 3: Remove duplicates (same random_key should be unique)\n",
    "print(\"\\n3Ô∏è‚É£ Removing duplicate member products...\")\n",
    "before_dedup = len(members_clean)\n",
    "members_clean = members_clean.drop_duplicates(subset=['random_key']).copy()\n",
    "dedup_removed = before_dedup - len(members_clean)\n",
    "print(f\"   Removed {dedup_removed:,} duplicate products\")\n",
    "\n",
    "print(f\"\\n‚úÖ Members table cleaned: {members_clean.shape}\")\n",
    "print(f\"   Total removed: {original_shapes['members'][0] - members_clean.shape[0]:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bbf9ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLEANING REFERENCE TABLES\n",
      "================================================================================\n",
      "\n",
      "üìÇ Cleaning categories...\n",
      "   ‚úÖ Categories: (746, 3)\n",
      "\n",
      "üè∑Ô∏è  Cleaning brands...\n",
      "   ‚úÖ Brands: (2025, 2)\n",
      "\n",
      "üèôÔ∏è  Cleaning cities...\n",
      "   ‚úÖ Cities: (651, 2)\n",
      "\n",
      "üè™ Cleaning shops...\n",
      "   Removed 0 shops with invalid city_id\n",
      "   ‚úÖ Shops: (23342, 4)\n",
      "\n",
      "‚úÖ All reference tables cleaned\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CLEANING REFERENCE TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get placeholder tokens from config (with fallback)\n",
    "placeholder_tokens = config.get('cleaning', {}).get('placeholder_tokens', \n",
    "                                                     ['none', 'null', 'nan', 'n/a', 'na', '-', '--', 'unknown', '?'])\n",
    "\n",
    "# Categories\n",
    "print(\"\\nüìÇ Cleaning categories...\")\n",
    "categories_clean = categories.copy()\n",
    "categories_clean['title'] = clean_text_field(categories_clean['title'], \n",
    "                                             placeholder_tokens=placeholder_tokens,\n",
    "                                             convert_to_none=True)\n",
    "categories_clean['title'] = normalize_persian_text(categories_clean['title'])\n",
    "categories_clean = categories_clean.drop_duplicates(subset=['id']).copy()\n",
    "print(f\"   ‚úÖ Categories: {categories_clean.shape}\")\n",
    "\n",
    "# Brands\n",
    "print(\"\\nüè∑Ô∏è  Cleaning brands...\")\n",
    "brands_clean = brands.copy()\n",
    "brands_clean['title'] = clean_text_field(brands_clean['title'],\n",
    "                                         placeholder_tokens=placeholder_tokens,\n",
    "                                         convert_to_none=True)\n",
    "# Brands can be in Persian or English, apply both normalizations\n",
    "brands_clean['title'] = normalize_persian_text(brands_clean['title'])\n",
    "brands_clean['title'] = normalize_english_text(brands_clean['title'])\n",
    "brands_clean = brands_clean.drop_duplicates(subset=['id']).copy()\n",
    "print(f\"   ‚úÖ Brands: {brands_clean.shape}\")\n",
    "\n",
    "# Cities\n",
    "print(\"\\nüèôÔ∏è  Cleaning cities...\")\n",
    "cities_clean = cities.copy()\n",
    "cities_clean['name'] = clean_text_field(cities_clean['name'],\n",
    "                                        placeholder_tokens=placeholder_tokens,\n",
    "                                        convert_to_none=True)\n",
    "cities_clean['name'] = normalize_persian_text(cities_clean['name'])\n",
    "cities_clean = cities_clean.drop_duplicates(subset=['id']).copy()\n",
    "print(f\"   ‚úÖ Cities: {cities_clean.shape}\")\n",
    "\n",
    "# Shops\n",
    "print(\"\\nüè™ Cleaning shops...\")\n",
    "shops_clean = shops.copy()\n",
    "# Remove shops that reference non-existent cities\n",
    "valid_city_ids = set(cities_clean['id'])\n",
    "before_shop_cleaning = len(shops_clean)\n",
    "shops_clean = shops_clean[shops_clean['city_id'].isin(valid_city_ids)].copy()\n",
    "shops_clean = shops_clean.drop_duplicates(subset=['id']).copy()\n",
    "shop_removed = before_shop_cleaning - len(shops_clean)\n",
    "print(f\"   Removed {shop_removed:,} shops with invalid city_id\")\n",
    "print(f\"   ‚úÖ Shops: {shops_clean.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ All reference tables cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e51e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLEANING USER INTERACTION TABLES\n",
      "================================================================================\n",
      "\n",
      "üîç Cleaning searches...\n",
      "   Removed 0 searches with empty queries\n",
      "   Removed 0 duplicate searches\n",
      "   ‚úÖ Searches: (588347, 9)\n",
      "\n",
      "üëÅÔ∏è  Cleaning base_views...\n",
      "   Removed 0 views with invalid search_id\n",
      "   Removed 12 views with invalid base_product_rk\n",
      "   Removed 0 duplicate views\n",
      "   ‚úÖ Base views: (199904, 4)\n",
      "\n",
      "üñ±Ô∏è  Cleaning final_clicks...\n",
      "   Removed 3 clicks with invalid base_view_id\n",
      "   Removed 0 clicks with invalid shop_id\n",
      "   Removed 0 duplicate clicks\n",
      "   ‚úÖ Final clicks: (17368, 4)\n",
      "\n",
      "‚úÖ All user interaction tables cleaned\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CLEANING USER INTERACTION TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get placeholder tokens from config (with fallback)\n",
    "placeholder_tokens = config.get('cleaning', {}).get('placeholder_tokens', \n",
    "                                                     ['none', 'null', 'nan', 'n/a', 'na', '-', '--', 'unknown', '?'])\n",
    "\n",
    "# Searches\n",
    "print(\"\\nüîç Cleaning searches...\")\n",
    "searches_clean = searches.copy()\n",
    "\n",
    "# Clean query text\n",
    "searches_clean['query'] = clean_text_field(searches_clean['query'],\n",
    "                                           placeholder_tokens=placeholder_tokens,\n",
    "                                           convert_to_none=True)\n",
    "searches_clean['query'] = normalize_persian_text(searches_clean['query'])\n",
    "\n",
    "# Remove searches with empty queries\n",
    "before_query_removal = len(searches_clean)\n",
    "searches_clean = searches_clean[searches_clean['query'].notna()].copy()\n",
    "query_removed = before_query_removal - len(searches_clean)\n",
    "print(f\"   Removed {query_removed:,} searches with empty queries\")\n",
    "\n",
    "# Remove duplicates\n",
    "before_dedup = len(searches_clean)\n",
    "searches_clean = searches_clean.drop_duplicates(subset=['id']).copy()\n",
    "dedup_removed = before_dedup - len(searches_clean)\n",
    "print(f\"   Removed {dedup_removed:,} duplicate searches\")\n",
    "\n",
    "print(f\"   ‚úÖ Searches: {searches_clean.shape}\")\n",
    "\n",
    "# Base Views\n",
    "print(\"\\nüëÅÔ∏è  Cleaning base_views...\")\n",
    "base_views_clean = base_views.copy()\n",
    "\n",
    "# Remove views referencing non-existent searches\n",
    "valid_search_ids = set(searches_clean['id'])\n",
    "before_view_cleaning = len(base_views_clean)\n",
    "base_views_clean = base_views_clean[base_views_clean['search_id'].isin(valid_search_ids)].copy()\n",
    "view_removed = before_view_cleaning - len(base_views_clean)\n",
    "print(f\"   Removed {view_removed:,} views with invalid search_id\")\n",
    "\n",
    "# Remove views referencing non-existent base products\n",
    "valid_base_keys = set(base_products_clean['random_key'])\n",
    "before_bp_removal = len(base_views_clean)\n",
    "base_views_clean = base_views_clean[base_views_clean['base_product_rk'].isin(valid_base_keys)].copy()\n",
    "bp_removed = before_bp_removal - len(base_views_clean)\n",
    "print(f\"   Removed {bp_removed:,} views with invalid base_product_rk\")\n",
    "\n",
    "# Remove duplicates\n",
    "before_dedup = len(base_views_clean)\n",
    "base_views_clean = base_views_clean.drop_duplicates(subset=['id']).copy()\n",
    "dedup_removed = before_dedup - len(base_views_clean)\n",
    "print(f\"   Removed {dedup_removed:,} duplicate views\")\n",
    "\n",
    "print(f\"   ‚úÖ Base views: {base_views_clean.shape}\")\n",
    "\n",
    "# Final Clicks\n",
    "print(\"\\nüñ±Ô∏è  Cleaning final_clicks...\")\n",
    "final_clicks_clean = final_clicks.copy()\n",
    "\n",
    "# Remove clicks referencing non-existent base views\n",
    "valid_view_ids = set(base_views_clean['id'])\n",
    "before_click_cleaning = len(final_clicks_clean)\n",
    "final_clicks_clean = final_clicks_clean[final_clicks_clean['base_view_id'].isin(valid_view_ids)].copy()\n",
    "click_removed = before_click_cleaning - len(final_clicks_clean)\n",
    "print(f\"   Removed {click_removed:,} clicks with invalid base_view_id\")\n",
    "\n",
    "# Remove clicks referencing non-existent shops\n",
    "valid_shop_ids = set(shops_clean['id'])\n",
    "before_shop_removal = len(final_clicks_clean)\n",
    "final_clicks_clean = final_clicks_clean[final_clicks_clean['shop_id'].isin(valid_shop_ids)].copy()\n",
    "shop_click_removed = before_shop_removal - len(final_clicks_clean)\n",
    "print(f\"   Removed {shop_click_removed:,} clicks with invalid shop_id\")\n",
    "\n",
    "# Remove duplicates\n",
    "before_dedup = len(final_clicks_clean)\n",
    "final_clicks_clean = final_clicks_clean.drop_duplicates(subset=['id']).copy()\n",
    "dedup_removed = before_dedup - len(final_clicks_clean)\n",
    "print(f\"   Removed {dedup_removed:,} duplicate clicks\")\n",
    "\n",
    "print(f\"   ‚úÖ Final clicks: {final_clicks_clean.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ All user interaction tables cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf56ee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA CLEANING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä CLEANING STATISTICS:\n",
      "      Dataset Original Rows Cleaned Rows Rows Removed Removed %  Original Cols  Cleaned Cols\n",
      "base_products     1,022,298    1,022,296            2     0.00%              8             9\n",
      "      members     1,948,665    1,948,015          650     0.03%              4             4\n",
      "     searches       588,347      588,347            0     0.00%              9             9\n",
      "   base_views       199,916      199,904           12     0.01%              4             4\n",
      " final_clicks        17,371       17,368            3     0.02%              4             4\n",
      "        shops        23,342       23,342            0     0.00%              4             4\n",
      "   categories           746          746            0     0.00%              3             3\n",
      "       brands         2,025        2,025            0     0.00%              2             2\n",
      "       cities           651          651            0     0.00%              2             2\n",
      "\n",
      "\n",
      "üìà KEY DATA QUALITY METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "Base Products:\n",
      "  - Products with persian_name: 1,022,296 (100.00%)\n",
      "  - Products with images: 1,018,955 (99.67%)\n",
      "  - Products with extra_features: 1,022,296 (100.00%)\n",
      "\n",
      "Members (Shop Products):\n",
      "  - Valid prices: 1,948,015 (100.00%)\n",
      "  - Price range: 500 - 99,999,999,999 Toman\n",
      "  - Mean price: 8,686,143 Toman\n",
      "  - Median price: 1,700,000 Toman\n",
      "\n",
      "Searches:\n",
      "  - Searches with queries: 588,347 (100.00%)\n",
      "  - Unique queries: 108,424\n",
      "\n",
      "Referential Integrity:\n",
      "  - All members link to valid base products: ‚úÖ\n",
      "  - All base_views link to valid searches: ‚úÖ\n",
      "  - All base_views link to valid base products: ‚úÖ\n",
      "  - All final_clicks link to valid base_views: ‚úÖ\n",
      "  - All final_clicks link to valid shops: ‚úÖ\n",
      "  - All shops link to valid cities: ‚úÖ\n",
      "\n",
      "‚úÖ Data cleaning completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_data = []\n",
    "\n",
    "datasets_comparison = [\n",
    "    ('base_products', original_shapes['base_products'], base_products_clean.shape),\n",
    "    ('members', original_shapes['members'], members_clean.shape),\n",
    "    ('searches', original_shapes['searches'], searches_clean.shape),\n",
    "    ('base_views', original_shapes['base_views'], base_views_clean.shape),\n",
    "    ('final_clicks', original_shapes['final_clicks'], final_clicks_clean.shape),\n",
    "    ('shops', original_shapes['shops'], shops_clean.shape),\n",
    "    ('categories', original_shapes['categories'], categories_clean.shape),\n",
    "    ('brands', original_shapes['brands'], brands_clean.shape),\n",
    "    ('cities', original_shapes['cities'], cities_clean.shape),\n",
    "]\n",
    "\n",
    "for name, orig_shape, clean_shape in datasets_comparison:\n",
    "    rows_removed = orig_shape[0] - clean_shape[0]\n",
    "    pct_removed = (rows_removed / orig_shape[0] * 100) if orig_shape[0] > 0 else 0\n",
    "    summary_data.append({\n",
    "        'Dataset': name,\n",
    "        'Original Rows': f\"{orig_shape[0]:,}\",\n",
    "        'Cleaned Rows': f\"{clean_shape[0]:,}\",\n",
    "        'Rows Removed': f\"{rows_removed:,}\",\n",
    "        'Removed %': f\"{pct_removed:.2f}%\",\n",
    "        'Original Cols': orig_shape[1],\n",
    "        'Cleaned Cols': clean_shape[1],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nüìä CLEANING STATISTICS:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Key data quality metrics\n",
    "print(\"\\n\\nüìà KEY DATA QUALITY METRICS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Base Products:\")\n",
    "print(f\"  - Products with persian_name: {base_products_clean['persian_name'].notna().sum():,} (100.00%)\")\n",
    "print(f\"  - Products with images: {base_products_clean['has_image'].sum():,} ({base_products_clean['has_image'].sum()/len(base_products_clean)*100:.2f}%)\")\n",
    "print(f\"  - Products with extra_features: {base_products_clean['extra_features_parsed'].apply(lambda x: len(x) > 0).sum():,} ({base_products_clean['extra_features_parsed'].apply(lambda x: len(x) > 0).sum()/len(base_products_clean)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nMembers (Shop Products):\")\n",
    "print(f\"  - Valid prices: {len(members_clean):,} (100.00%)\")\n",
    "print(f\"  - Price range: {members_clean['price'].min():,.0f} - {members_clean['price'].max():,.0f} Toman\")\n",
    "print(f\"  - Mean price: {members_clean['price'].mean():,.0f} Toman\")\n",
    "print(f\"  - Median price: {members_clean['price'].median():,.0f} Toman\")\n",
    "\n",
    "print(f\"\\nSearches:\")\n",
    "print(f\"  - Searches with queries: {searches_clean['query'].notna().sum():,} (100.00%)\")\n",
    "print(f\"  - Unique queries: {searches_clean['query'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nReferential Integrity:\")\n",
    "print(f\"  - All members link to valid base products: ‚úÖ\")\n",
    "print(f\"  - All base_views link to valid searches: ‚úÖ\")\n",
    "print(f\"  - All base_views link to valid base products: ‚úÖ\")\n",
    "print(f\"  - All final_clicks link to valid base_views: ‚úÖ\")\n",
    "print(f\"  - All final_clicks link to valid shops: ‚úÖ\")\n",
    "print(f\"  - All shops link to valid cities: ‚úÖ\")\n",
    "\n",
    "print(\"\\n‚úÖ Data cleaning completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91fe6af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING CLEANED DATA\n",
      "================================================================================\n",
      "\n",
      "üíæ Saving to: ../Data/processed\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Saved: base_products_clean.parquet (1,022,296 rows)\n",
      "‚úÖ Saved: members_clean.parquet (1,947,743 rows)\n",
      "‚úÖ Saved: searches_clean.parquet (588,347 rows)\n",
      "‚úÖ Saved: base_views_clean.parquet (199,904 rows)\n",
      "‚úÖ Saved: final_clicks_clean.parquet (17,368 rows)\n",
      "‚úÖ Saved: shops_clean.parquet (23,342 rows)\n",
      "‚úÖ Saved: categories_clean.parquet (746 rows)\n",
      "‚úÖ Saved: brands_clean.parquet (2,025 rows)\n",
      "‚úÖ Saved: cities_clean.parquet (651 rows)\n",
      "\n",
      "‚úÖ Saved: cleaning_metadata.json\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ALL DATA SAVED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "  1. Use cleaned data for Feature Engineering\n",
      "  2. Extract embeddings from text and images\n",
      "  3. Build RAG retrieval system\n",
      "\n",
      "Cleaned data location: ../Data/processed\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAVING CLEANED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save cleaned datasets\n",
    "output_path = '../Data/processed'\n",
    "\n",
    "print(f\"\\nüíæ Saving to: {output_path}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Save each table\n",
    "base_products_clean.to_parquet('../Data/processed/base_products_clean.parquet', index=False)\n",
    "print(f\"‚úÖ Saved: base_products_clean.parquet ({base_products_clean.shape[0]:,} rows)\")\n",
    "\n",
    "members_clean.to_parquet('../Data/processed/members_clean.parquet', index=False)\n",
    "print(f\"‚úÖ Saved: members_clean.parquet ({members_clean.shape[0]:,} rows)\")\n",
    "\n",
    "searches_clean.to_parquet('../Data/processed/searches_clean.parquet', index=False)\n",
    "print(f\"‚úÖ Saved: searches_clean.parquet ({searches_clean.shape[0]:,} rows)\")\n",
    "\n",
    "base_views_clean.to_parquet('../Data/processed/base_views_clean.parquet', index=False)\n",
    "print(f\"‚úÖ Saved: base_views_clean.parquet ({base_views_clean.shape[0]:,} rows)\")\n",
    "\n",
    "final_clicks_clean.to_parquet('../Data/processed/final_clicks_clean.parquet', index=False)\n",
    "print(f\"‚úÖ Saved: final_clicks_clean.parquet ({final_clicks_clean.shape[0]:,} rows)\")\n",
    "\n",
    "shops_clean.to_parquet('../Data/processed/shops_clean.parquet', index=False)\n",
    "print(f\"‚úÖ Saved: shops_clean.parquet ({shops_clean.shape[0]:,} rows)\")\n",
    "\n",
    "categories_clean.to_parquet('../Data/processed/categories_clean.parquet', index=False)\n",
    "print(f\"‚úÖ Saved: categories_clean.parquet ({categories_clean.shape[0]:,} rows)\")\n",
    "\n",
    "brands_clean.to_parquet('../Data/processed/brands_clean.parquet', index=False)\n",
    "print(f\"‚úÖ Saved: brands_clean.parquet ({brands_clean.shape[0]:,} rows)\")\n",
    "\n",
    "cities_clean.to_parquet('../Data/processed/cities_clean.parquet', index=False)\n",
    "print(f\"‚úÖ Saved: cities_clean.parquet ({cities_clean.shape[0]:,} rows)\")\n",
    "\n",
    "# Save cleaning metadata\n",
    "metadata = {\n",
    "    'cleaning_date': datetime.now().isoformat(),\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'config': config,\n",
    "    'original_shapes': {k: list(v) for k, v in original_shapes.items()},\n",
    "    'cleaned_shapes': {\n",
    "        'base_products': list(base_products_clean.shape),\n",
    "        'members': list(members_clean.shape),\n",
    "        'searches': list(searches_clean.shape),\n",
    "        'base_views': list(base_views_clean.shape),\n",
    "        'final_clicks': list(final_clicks_clean.shape),\n",
    "        'shops': list(shops_clean.shape),\n",
    "        'categories': list(categories_clean.shape),\n",
    "        'brands': list(brands_clean.shape),\n",
    "        'cities': list(cities_clean.shape),\n",
    "    },\n",
    "    'summary': {\n",
    "        'total_rows_removed': sum([\n",
    "            original_shapes['base_products'][0] - base_products_clean.shape[0],\n",
    "            original_shapes['members'][0] - members_clean.shape[0],\n",
    "            original_shapes['searches'][0] - searches_clean.shape[0],\n",
    "            original_shapes['base_views'][0] - base_views_clean.shape[0],\n",
    "            original_shapes['final_clicks'][0] - final_clicks_clean.shape[0],\n",
    "            original_shapes['shops'][0] - shops_clean.shape[0],\n",
    "            original_shapes['categories'][0] - categories_clean.shape[0],\n",
    "            original_shapes['brands'][0] - brands_clean.shape[0],\n",
    "            original_shapes['cities'][0] - cities_clean.shape[0],\n",
    "        ])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../Data/processed/cleaning_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "print(f\"\\n‚úÖ Saved: cleaning_metadata.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ ALL DATA SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Use cleaned data for Feature Engineering\")\n",
    "print(f\"  2. Extract embeddings from text and images\")\n",
    "print(f\"  3. Build RAG retrieval system\")\n",
    "print(f\"\\nCleaned data location: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
